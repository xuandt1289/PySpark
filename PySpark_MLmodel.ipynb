{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark - ML model\n",
    "\n",
    "This notebook is based on https://github.com/jadianes/spark-py-notebooks\n",
    "\n",
    "It is about to build a movie recommendation model (Collaborative Filtering) using public MovieLens dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "\n",
    "GroupLens Research has collected and made available rating data sets from the MovieLens website. The data sets were collected over various periods of time, depending on the size of the set. \n",
    "\n",
    "In our case, we will use the lastest datasets:\n",
    "- Small dataset \n",
    "- Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links of datasets\n",
    "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "\n",
    "# locations' paths store datasets\n",
    "import os\n",
    "datasets_path = os.path.join('C:/Users/xuand', 'datasets')\n",
    "\n",
    "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
    "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')\n",
    "\n",
    "# download datasets\n",
    "import urllib.request\n",
    "\n",
    "small_f = urllib.request.urlretrieve (small_dataset_url, small_dataset_path)\n",
    "complete_f = urllib.request.urlretrieve (complete_dataset_url, complete_dataset_path)\n",
    "\n",
    "# Extract these downloaded files into its individual folders\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(small_dataset_path, 'r') as z:\n",
    "    z.extractall(datasets_path)\n",
    "with zipfile.ZipFile(complete_dataset_path, 'r') as z:\n",
    "    z.extractall(datasets_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RDD files\n",
    "\n",
    "### Create a PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('MLmodel-PySpark').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD files for ratings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1', '4.0'), ('1', '3', '4.0'), ('1', '6', '4.0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create RDD files\n",
    "import os\n",
    "datasets_path = os.path.join('C:/Users/xuand', 'datasets')\n",
    "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
    "\n",
    "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]\n",
    "\n",
    "small_ratings_data = small_ratings_raw_data.filter(lambda line: line != small_ratings_raw_data_header).map(lambda line: line.split(',')).map(lambda tokens: (tokens[0], tokens[1], tokens[2])).cache()\n",
    "small_ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD files for movies file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Toy Story (1995)'),\n",
       " ('2', 'Jumanji (1995)'),\n",
       " ('3', 'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
    "\n",
    "small_movies_raw_data = sc.textFile(small_movies_file)\n",
    "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
    "\n",
    "small_movies_data = small_movies_raw_data.filter(lambda line: line != small_movies_raw_data_header).map(lambda line: line.split(',')).map(lambda tokens: (tokens[0], tokens[1])).cache()\n",
    "small_movies_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering (CF)\n",
    "In CF, we make predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption is that if a user A has the same option as a user B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a user chosen randomly.\n",
    "\n",
    "The image below shows an example of CF. At first, people rate different items, then the system makes predictions about a user's rating for an item not rated yet. The new predictions are built upon the existing ratings of other users with similar ratings with the active user. In the image, the system predicts that the user will not like the video.\n",
    "\n",
    "### CF in MLlib \n",
    "Spark MLlib library for ML provides CF implementation by using Alternating Least Squares (ALS). The implementation in MLlib has the following parameters:\n",
    "\n",
    "- numBlocks is the number of blocks used to parallelize computaiton (set to -1 to auto-configure)\n",
    "- rank is the number of latent factors in the model\n",
    "- iterations is the number of iterations to run\n",
    "- lambda specifies the regularization parameter in ALS\n",
    "- implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data\n",
    "- alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using small dataset to select ALS parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.8973056100718643\n",
      "For rank 8 the RMSE is 0.9143149069672253\n",
      "For rank 12 the RMSE is 0.9141049207539428\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train, validation, and test datasets\n",
    "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6,2,2], seed=0)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "seed = 5\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print('The best model was trained with rank %s' %best_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for testing data, the RMSE is 0.9041156142407426\n"
     ]
    }
   ],
   "source": [
    "# test the selected model\n",
    "model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,lambda_=regularization_parameter)\n",
    "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "print('for testing data, the RMSE is %s' %error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the complete dataset to build the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27753444 recommendations in the complete dataset\n",
      "For testing data the RMSE is 0.8334782410156314\n"
     ]
    }
   ],
   "source": [
    "# load the complete dataset file\n",
    "complete_ratings_file = os.path.join(datasets_path, 'ml-latest', 'ratings.csv')\n",
    "complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
    "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
    "\n",
    "# parse\n",
    "complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line != complete_ratings_raw_data_header).map(lambda line: line.split(',')).map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))).cache()\n",
    "print('There are %s recommendations in the complete dataset' %(complete_ratings_data.count()))\n",
    "\n",
    "# split dataset into train and test datasets\n",
    "training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)\n",
    "\n",
    "complete_model = ALS.train(training_RDD, best_rank, seed=seed,iterations=iterations, lambda_=regularization_parameter)\n",
    "\n",
    "# test the selected model\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print('For testing data the RMSE is %s' % (error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new user ratings\n",
    "\n",
    "We will put them in a new RDD and we will use the user ID 0, that is not assigned in the MovieLens dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)]\n"
     ]
    }
   ],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "     (0,260,9), # Star Wars (1977)\n",
    "     (0,1,8), # Toy Story (1995)\n",
    "     (0,16,7), # Casino (1995)\n",
    "     (0,25,8), # Leaving Las Vegas (1995)\n",
    "     (0,32,9), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
    "     (0,335,4), # Flintstones, The (1994)\n",
    "     (0,379,3), # Timecop (1994)\n",
    "     (0,296,7), # Pulp Fiction (1994)\n",
    "     (0,858,10) , # Godfather, The (1972)\n",
    "     (0,50,8) # Usual Suspects, The (1995)\n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print('New user ratings: %s' % new_user_ratings_RDD.take(10))\n",
    "\n",
    "# add new user to the data\n",
    "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)\n",
    "\n",
    "# train ALS model using new data set\n",
    "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, \n",
    "                              iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took some time. We will need to repeat that every time a user add new ratings. Ideally we will do this in batches, and not for every single rating that comes into the system for every user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting top recommendations\n",
    "Let's now get some recommendations! For that we will get an RDD with all the movies the new user hasn't rated yet. We will them together with the model to predict ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58098 movies in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "# load and process movies file\n",
    "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
    "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
    "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
    "\n",
    "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))\n",
    "    \n",
    "print(\"There are %s movies in the complete dataset\" % (complete_movies_titles.count()))\n",
    "\n",
    "def get_counts_and_averages(ID_and_ratings_tuple):\n",
    "    nratings = len(ID_and_ratings_tuple[1])\n",
    "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
    "\n",
    "movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))\n",
    "\n",
    "# new_user_recommendations_RDD\n",
    "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
    "# keep just those not on the ID list (thanks Lei Li for spotting the error!)\n",
    "new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "\n",
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)\n",
    "\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get the highest rated recommendations for the new user, filtering out movies with less than 10 ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP recommended movies (with more than 25 reviews):\n",
      "('Rabbit of Seville (1950)', 8.60283332885939, 30)\n",
      "(\"Jim Henson's The Storyteller (1989)\", 8.544291232021493, 36)\n",
      "('\"Godfather', 8.52628915469041, 60904)\n",
      "('Cosmos', 8.52467364564589, 157)\n",
      "('Death on the Staircase (SoupÃ§ons) (2004)', 8.521656039332541, 130)\n",
      "('Music for One Apartment and Six Drummers (2001)', 8.502821262499474, 31)\n",
      "('\"Human Condition III', 8.495127363853982, 91)\n",
      "('Harakiri (Seppuku) (1962)', 8.494578606156566, 679)\n",
      "('\"I', 8.424360308543928, 85)\n",
      "('Planet Earth (2006)', 8.424323803290726, 1384)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting individual rating for a particular movie for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=116688, rating=2.070225567265668)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
